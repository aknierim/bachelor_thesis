\chapter{Conclusions and Outlook}
\label{ch:conclusions}

In the scope of this work, the hyperparameters for each of the four currently (as of writing this thesis)
implemented cleaning algorithms were searched via a grid search. The resulting hyperparameters were then
probed \wrt a combined metric of the angular resolution and the efficiency. These results led to a subset
consisting of the best-performing hyperparameters for each cleaning algorithm. Furthermore, a comparison between the
optimized algorithms and the default algorithms, as well as a comparison between each respective
cleaner was performed.

Most of the data (pre-)processing in this work was done with \gls{cta}'s open-source low-level data processing pipeline
software \ctapipe{}, version \texttt{0.15.1}. The data used was PROD5 \gls{mc} simulations consisting of
\(\num{1.3}\)\,M telescope events (\sim\(\num{620}\)\,k array events) throughout \(987\) runs. The data
was processed from raw \texttt{simtel} data (\rzero) up to cleaned (\dloa) and parametrized (\dlob) levels
for the metrics as well as reconstructed (\dlt) data for the efficiency and angular resolution.
Further high-level processing was done with this works pipeline as described in \autoref{sec:pipeline}.

The four implemented cleaning algorithms can be roughly categorized into time-based (\fact{} and \tcc) and non-time-based
algorithms (\tailcuts{} and \mars{}). While the non-time-based algorithms only take thresholds of the photon count of each pixel
into account, the time-based algorithms also set limits on the arrival time of each pixel. This allows
for a lower core threshold \(Q_c\) for the photon count per pixel, which in turn can lead to better
results in the cleaning process. A good cleaning not only reduces data size---as a majority of the
pixels in the image are made up by \gls{nsb}---but it also allows for a better parameterization and in turn
a better reconstruction of the events.

The main results of this thesis are the hyperparameters for the cleaning algorithms.
Due to the long runtimes of the grid searches and the time limitation of this thesis, however, the hyperparameters
for cleaning algorithms were only optimized for \glspl{mst}. The optimal hyperparameters of each cleaning algorithm for
the \glspl{mst} are listed in \autoref{tab:best_parameters}. A further grid search with only \gls{lst}
data would be necessary to find the remaining core and boundary thresholds \(Q_c\) and \(Q_b\) as well
as the time limits for the \gls{lst} data.

The comparison of the performance of the optimized algorithms with the default algorithms has shown that
the optimized algorithms performed better than the default algorithms \wrt the angular resolution. This
means that the found hyperparameters could help with origin reconstructions. When looking at the metrics,
however, only \tcc{} really improved, while \fact{} even performed worse than its default implementation.
The reason might be, that the higher core threshold \(Q_c\)---compared to the default value---would
select fewer pixels. Additionally, a comparison between the optimized algorithms themselves was performed. This led to \fact{}
being the best performing algorithm overall \wrt the metrics, with only a slight advantage over the
other algorithms, however.

All in all, one can say, that the hyperparameters determined at a higher efficiency return better results,
albeit resulting in a higher angular resolution. This is due to the fact that a higher efficiency means
that more events got reconstructed and therefore better overall statistics. Nevertheless, further analysis
and a broader grid search might be necessary to find better hyperparameters
and therefore better the performance of all algorithms. This will also be necessary for the \gls{lst} data.

